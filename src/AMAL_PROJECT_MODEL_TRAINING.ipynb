{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hBmVA5ntLW5",
        "outputId": "a350604b-ad36-4cf4-c762-8b21e2f42dc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jNP_dl6Utb3Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import math\n",
        "from math import sqrt\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from scipy.stats import gmean\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uKATRfEmthG4"
      },
      "outputs": [],
      "source": [
        "class SinAct(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "class CustomBatchNorm1d(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm1d(d, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(x)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import sqrt\n",
        "\n",
        "class CustomNormalization(nn.Module):\n",
        "    def __init__(self, norm_type, mean_reduction, force_factor=None):\n",
        "        \"\"\"\n",
        "        Initializes the CustomNormalization layer.\n",
        "\n",
        "        Args:\n",
        "            norm_type (str): Type of normalization to apply. 'bn' for batch normalization,\n",
        "                             'ln' for layer normalization, 'id' for identity (no normalization).\n",
        "            mean_reduction (bool): If True, subtracts the mean before normalization.\n",
        "            force_factor (float, optional): A custom scaling factor. If None, it's computed based on dimensions.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mean_reduction = mean_reduction\n",
        "        self.norm_type = norm_type\n",
        "        self.force_factor = force_factor\n",
        "\n",
        "        # Determines the dimension along which normalization is applied.\n",
        "        if norm_type == 'bn':\n",
        "            self.dim = 0  # Normalize across the batch size (columns).\n",
        "        elif norm_type == 'ln':\n",
        "            self.dim = 1  # Normalize across the feature dimension (rows).\n",
        "        elif norm_type == 'id':\n",
        "            self.dim = -1  # No normalization.\n",
        "        else:\n",
        "            raise ValueError(\"No such normalization.\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Applies the normalization to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            X (Tensor): The input tensor to normalize.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The normalized tensor.\n",
        "        \"\"\"\n",
        "        # If 'id', return the input as is (no normalization).\n",
        "        if self.dim == -1:\n",
        "            return X\n",
        "\n",
        "        # If mean_reduction is True, subtracts the mean from the tensor along the specified dimension.\n",
        "        if self.mean_reduction:\n",
        "            X = X - X.mean(dim=self.dim, keepdim=True)\n",
        "\n",
        "        # Computes the norm of the tensor along the specified dimension.\n",
        "        norm = X.norm(dim=self.dim, keepdim=True)\n",
        "\n",
        "        # Determines the scaling factor: the square root of the dimension size.\n",
        "        # For batch normalization ('bn'), it's the batch size (n).\n",
        "        # For layer normalization ('ln'), it's the feature dimension size (d).\n",
        "        factor = sqrt(X.shape[self.dim])\n",
        "\n",
        "        # If a custom force_factor is provided, it overrides the computed factor.\n",
        "        if self.force_factor is not None:\n",
        "            factor = self.force_factor\n",
        "\n",
        "        # Normalizes the tensor by dividing each element by (norm / factor).\n",
        "        X = X / (norm / factor)\n",
        "        return X\n",
        "\n",
        "\n",
        "class GainedActivation(nn.Module):\n",
        "    def __init__(self, activation, gain):\n",
        "        super().__init__()\n",
        "        self.activation = activation()\n",
        "        self.gain = nn.Parameter(torch.tensor([gain], requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.gain * x)\n",
        "\n",
        "\n",
        "\n",
        "###############################\n",
        "\n",
        "\n",
        "class MLPWithBatchNorm(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_layers, hidden_dim, norm_type, mean_reduction, activation, save_hidden, exponent, order='norm_act', force_factor=None, bias=False):\n",
        "        \"\"\"\n",
        "        Initializes the MLPWithBatchNorm class.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Dimension of the input features.\n",
        "            output_dim (int): Dimension of the output.\n",
        "            num_layers (int): Number of layers in the MLP.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "            norm_type (str): Type of normalization ('torch_bn' for PyTorch BatchNorm1d or other types for custom normalization).\n",
        "            mean_reduction (bool): If True, normalization includes mean reduction.\n",
        "            activation (callable): Activation function to be used in the network.\n",
        "            save_hidden (bool): If True, saves the output of each layer.\n",
        "            exponent (float): Exponent factor for layer gain adjustment.\n",
        "            order (str): The order of applying normalization and activation. Either 'act_norm' or 'norm_act'.\n",
        "            force_factor (float, optional): Force factor for custom normalization.\n",
        "            bias (bool): If True, adds a learnable bias to the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.hiddens = {}  # Dictionary to store outputs of each layer if save_hidden is True.\n",
        "        self.initialized = False  # Flag to check if the model's parameters have been initialized.\n",
        "        self.exponent = exponent\n",
        "        self.save_hidden = save_hidden\n",
        "        self.order = order\n",
        "\n",
        "        # Validate the order of normalization and activation.\n",
        "        if self.order not in ['act_norm', 'norm_act']:\n",
        "            raise ValueError(\"Unknown order\")\n",
        "\n",
        "        # Initializing the layers of the MLP.\n",
        "        self.layers = nn.ModuleDict()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers[f'fc_0'] = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
        "        # Normalization layer\n",
        "        if norm_type == 'torch_bn':\n",
        "            self.layers[f'norm_0'] = nn.BatchNorm1d(hidden_dim)\n",
        "        else:\n",
        "            self.layers[f'norm_0'] = CustomNormalization(norm_type, mean_reduction, force_factor=force_factor)\n",
        "        # Activation layer\n",
        "        self.layers[f'act_0'] = activation()\n",
        "\n",
        "        # Hidden layers\n",
        "        for l in range(1, num_layers):\n",
        "            self.layers[f'fc_{l}'] = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
        "            if norm_type == 'torch_bn':\n",
        "                self.layers[f'norm_{l}'] = nn.BatchNorm1d(hidden_dim)\n",
        "            else:\n",
        "                self.layers[f'norm_{l}'] = CustomNormalization(norm_type, mean_reduction, force_factor=force_factor)\n",
        "            self.layers[f'act_{l}'] = activation()\n",
        "\n",
        "        # Output layer\n",
        "        self.layers[f'fc_{num_layers}'] = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor after passing through the MLP.\n",
        "        \"\"\"\n",
        "        assert self.initialized, \"Model parameters not initialized.\"\n",
        "\n",
        "        # Flatten the input tensor if necessary.\n",
        "        x = x.view(-1, self.input_dim)\n",
        "\n",
        "        # Pass input through each layer.\n",
        "        for l in range(self.num_layers):\n",
        "            # Calculate layer gain based on the exponent.\n",
        "            layer_gain = ((l + 1) ** self.exponent)\n",
        "\n",
        "            # Apply linear transformation.\n",
        "            x = self.layers[f'fc_{l}'](x)\n",
        "            if self.save_hidden:\n",
        "                self.hiddens[f'fc_{l}'] = x.clone().detach()\n",
        "\n",
        "            # Apply normalization and activation in the specified order.\n",
        "            if self.order == 'norm_act':\n",
        "                x = self.layers[f'norm_{l}'](x)\n",
        "                if self.save_hidden:\n",
        "                    self.hiddens[f'norm_{l}'] = x.clone().detach()\n",
        "                x = self.layers[f'act_{l}'](x * layer_gain)\n",
        "                if self.save_hidden:\n",
        "                    self.hiddens[f'act_{l}'] = x.clone().detach()\n",
        "            elif self.order == 'act_norm':\n",
        "                x = self.layers[f'act_{l}'](x * layer_gain)\n",
        "                if self.save_hidden:\n",
        "                    self.hiddens[f'act_{l}'] = x.clone().detach()\n",
        "                x = self.layers[f'norm_{l}'](x)\n",
        "                if self.save_hidden:\n",
        "                    self.hiddens[f'norm_{l}'] = x.clone().detach()\n",
        "\n",
        "        # Final layer to produce output.\n",
        "        x = self.layers[f'fc_{self.num_layers}'](x)\n",
        "        if self.save_hidden:\n",
        "            self.hiddens[f'fc_{self.num_layers}'] = x.clone().detach()\n",
        "        return x\n",
        "\n",
        "    def set_save_hidden(self, state):\n",
        "        \"\"\"\n",
        "        Enables or disables saving of hidden layer outputs.\n",
        "\n",
        "        Args:\n",
        "            state (bool): If True, enables saving hidden layer outputs.\n",
        "        \"\"\"\n",
        "        self.save_hidden = state\n",
        "        if state:\n",
        "            self.hiddens.clear()\n",
        "\n",
        "    def reset_parameters(self, init_type, gain=1.0):\n",
        "        \"\"\"\n",
        "        Resets the parameters of the network according to the specified initialization type.\n",
        "\n",
        "        Args:\n",
        "            init_type (str): Type of initialization ('xavier_normal' or 'orthogonal').\n",
        "            gain (float): Gain factor for initialization.\n",
        "        \"\"\"\n",
        "        for name, p in self.named_modules():\n",
        "            if isinstance(p, nn.Linear):\n",
        "                # Xavier normal initialization.\n",
        "                if init_type == 'xavier_normal':\n",
        "                    nn.init.xavier_normal_(p.weight, gain=gain)\n",
        "                # Orthogonal initialization.\n",
        "                elif init_type == 'orthogonal':\n",
        "                    nn.init.orthogonal_(p.weight)\n",
        "                else:\n",
        "                    raise ValueError(\"No such initialization scheme.\")\n",
        "        self.initialized = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NRsjvEu6tp-b"
      },
      "outputs": [],
      "source": [
        "def generate_matrix_close_to_isometry(d):\n",
        "    X = torch.rand(d, d)\n",
        "    eps = torch.rand(d)\n",
        "    eps /= eps.sum()\n",
        "    eps -= 1/d\n",
        "\n",
        "    eigs = (1 + eps).sqrt()\n",
        "    U, S, Vt = torch.linalg.svd(X)\n",
        "    return U @ eigs.diag() @ Vt\n",
        "\n",
        "def generate_matrix_far_from_isometry(d, eps):\n",
        "    X = torch.rand(d, d)\n",
        "    eigs = torch.from_numpy(np.asarray([d - (d-1) * eps] + [eps] * (d-1))).float().sqrt()\n",
        "    U, S, Vt = torch.linalg.svd(X)\n",
        "    return U @ eigs.diag() @ Vt\n",
        "\n",
        "def cosine(x, y):\n",
        "    return torch.dot(x, y) / (x.norm().abs() * y.norm().abs())\n",
        "\n",
        "def cosine_similarity(x, y):\n",
        "    return 1 - cosine(x, y).abs()\n",
        "\n",
        "def isometry_gap(X):\n",
        "    G = X @ X.t()\n",
        "    G = G.detach().cpu()\n",
        "    eigs = torch.linalg.eigvalsh(G)\n",
        "    return -torch.log(eigs).mean() + torch.log(torch.mean(eigs))\n",
        "\n",
        "def ortho_gap(X):\n",
        "    n, d = X.shape\n",
        "    I_n = torch.eye(n).to(X.device)\n",
        "    Y1 = (X@X.T) / (X.norm('fro')**2)\n",
        "    Y2 = I_n / (I_n.norm('fro')**2)\n",
        "    return (Y1 - Y2).norm('fro')\n",
        "\n",
        "def isometry_gap2(X):\n",
        "    G = X @ X.t()\n",
        "    G = G.detach().cpu()\n",
        "    eigs = torch.linalg.eigvalsh(G).numpy()\n",
        "    return -np.log(gmean(eigs) / eigs.mean())\n",
        "\n",
        "\n",
        "def get_measurements(model, inputs, labels, criterion, epoch, device):\n",
        "    # Do one forward pass and one backward pass without updating anything\n",
        "    model.train()\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.set_save_hidden(True)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    measurements = []\n",
        "    for l in tqdm.tqdm(range(0, model.num_layers+1)):\n",
        "        w = model.layers[f'fc_{l}'].weight\n",
        "        w_grad = model.layers[f'fc_{l}'].weight.grad\n",
        "        w_grad_fro = torch.linalg.matrix_norm(w_grad, ord='fro').item()\n",
        "        w_ig = isometry_gap(w).item()\n",
        "        fc_ig = isometry_gap(model.hiddens[f'fc_{l}']).item()\n",
        "        if l < model.num_layers:\n",
        "            act_ig = isometry_gap(model.hiddens[f'act_{l}']).item()\n",
        "            norm_ig = isometry_gap(model.hiddens[f'norm_{l}']).item()\n",
        "        else:\n",
        "            # Final layer\n",
        "            act_ig = np.nan\n",
        "            norm_ig = np.nan\n",
        "        measurements.append({\n",
        "            'layer': l,\n",
        "            'epoch': epoch,\n",
        "            'weight_isogap': w_ig,\n",
        "            'fc_isogap': fc_ig,\n",
        "            'act_isogap': act_ig,\n",
        "            'norm_isogap': norm_ig,\n",
        "            'grad_fro_norm': w_grad_fro,\n",
        "        })\n",
        "\n",
        "    # Sanity cleaning gradients\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    model.set_save_hidden(False)\n",
        "    return measurements\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        train_accuracy = correct / total\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if math.isnan(running_loss):\n",
        "            print(\"Train loss is nan\", flush=True, file=sys.stderr)\n",
        "            exit(1)\n",
        "\n",
        "    train_accuracy = correct / total\n",
        "    train_loss = running_loss / len(loader)\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "def test_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            test_accuracy = correct / total\n",
        "\n",
        "            if math.isnan(running_loss):\n",
        "                print(\"Test loss is nan\", flush=True, file=sys.stderr)\n",
        "                exit(1)\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    test_loss = running_loss / len(loader)\n",
        "    return test_loss, test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w5ylmI6jt0PE"
      },
      "outputs": [],
      "source": [
        "def dataset_to_tensors(dataset, indices=None, device='cuda'):\n",
        "    if indices is None:\n",
        "        indices = range(len(dataset))  # all\n",
        "    xy_train = [dataset[i] for i in indices]\n",
        "    x = torch.stack([e[0] for e in xy_train]).to(device)\n",
        "    y = torch.stack([torch.tensor(e[1]) for e in xy_train]).to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "class TensorDataLoader:\n",
        "    \"\"\"Combination of torch's DataLoader and TensorDataset for efficient batch sampling\n",
        "    and adaptive augmentation on GPU.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        x,\n",
        "        y,\n",
        "        batch_size=500,\n",
        "        shuffle=False,\n",
        "    ):\n",
        "        assert x.size(0) == y.size(0), 'Size mismatch'\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.device = x.device\n",
        "        self.n_data = y.size(0)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_batches = ceil(self.n_data / self.batch_size)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            permutation = torch.randperm(self.n_data, device=self.device)\n",
        "            self.x = self.x[permutation]\n",
        "            self.y = self.y[permutation]\n",
        "        self.i_batch = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i_batch >= self.n_batches:\n",
        "            raise StopIteration\n",
        "\n",
        "        start = self.i_batch * self.batch_size\n",
        "        end = start + self.batch_size\n",
        "        x, y = self.x[start:end], self.y[start:end]\n",
        "        self.i_batch += 1\n",
        "        return (x, y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "    def attach(self):\n",
        "        self._detach = False\n",
        "        return self\n",
        "\n",
        "    def detach(self):\n",
        "        self._detach = True\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def dataset(self):\n",
        "        return DatasetDummy(self.n_data)\n",
        "\n",
        "\n",
        "class DatasetDummy:\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(self.N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jkL4grzjt7vG"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "DS_INPUT_SIZES = {\n",
        "    'CIFAR10': 3 * 32 * 32,\n",
        "    'CIFAR100': 3 * 32 * 32,\n",
        "    'MNIST': 28 * 28,\n",
        "    'FashionMNIST': 28 * 28,\n",
        "}\n",
        "DS_NUM_CLASSES = {\n",
        "    'CIFAR10': 10,\n",
        "    'CIFAR100': 100,\n",
        "    'MNIST': 10,\n",
        "    'FashionMNIST': 10,\n",
        "}\n",
        "\n",
        "DS_TRANSFORMS = {\n",
        "    'CIFAR10': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))]),\n",
        "    'CIFAR100': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762))]),\n",
        "    'MNIST': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307), (0.3081))]),\n",
        "    'FashionMNIST': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.2860), (0.3530))]),\n",
        "}\n",
        "\n",
        "ACTIVATIONS = {\n",
        "    'identity': nn.Identity,\n",
        "    'sin': SinAct,\n",
        "    'tanh': nn.Tanh,\n",
        "    'selu': nn.SELU,\n",
        "    'relu': nn.ReLU,\n",
        "    'leaky_relu': nn.LeakyReLU\n",
        "}\n",
        "\n",
        "GAINS = {\n",
        "    'identity': 1,\n",
        "    'sin': 1,\n",
        "    'tanh': 5/3,\n",
        "    'selu': 3/4,\n",
        "    'relu': np.sqrt(2),\n",
        "    'leaky_relu': nn.init.calculate_gain('leaky_relu')\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBq_xi1IuTZ5"
      },
      "source": [
        "# Setup configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XY9iHJU9uUtz"
      },
      "outputs": [],
      "source": [
        "# Configuration setup\n",
        "config = {\n",
        "    'dataset': 'MNIST',\n",
        "    'num_layers': None, #TBD\n",
        "    'hidden_dim': 100,\n",
        "    'batch_size': 100,\n",
        "    'init_type': None, # TBD\n",
        "    'norm_type': 'torch_bn',\n",
        "    'activation': None, # TBD\n",
        "    'learning_rate': 0.001,\n",
        "    'order': 'norm_act',\n",
        "    'bias': True,\n",
        "    'mean_reduction': None, # Not needed if we use default pytorch BN\n",
        "    'force_factor': None, # Not needed if we use default pytorch BN\n",
        "    'gain_exponent': -0.4,\n",
        "    'num_epochs': 1000\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV-gT_7GuXh6"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tgHtR66DuYqS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "ds = getattr(torchvision.datasets, config['dataset'])\n",
        "transform = torchvision.transforms.ToTensor()  # Replace with actual transform\n",
        "trainset = ds(root='../Data', train=True, download=True, transform=transform)\n",
        "testset = ds(root='../Data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = TensorDataLoader(*dataset_to_tensors(trainset, device=device), batch_size=config['batch_size'], shuffle=True)\n",
        "testloader = TensorDataLoader(*dataset_to_tensors(testset, device=device), batch_size=config['batch_size'], shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA6YOkkSIPpS"
      },
      "source": [
        "# To run all night"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EsWXd6IIRqr",
        "outputId": "c96ba53b-d1e2-44b5-e2b3-838d508cc027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 12, Train Loss: 1.1155, Train Acc: 0.8422, Test Loss: 1.4470, Test Acc: 0.8723:   1%|          | 12/1000 [04:30<6:07:38, 22.33s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the combinations of parameters : RUN ONLY ONE COMBINATION PER COLAB TO PARALLELIZE\n",
        "combinations = [\n",
        "        \n",
        "    # ('orthogonal', 100, 'identity'),\n",
        "    # ('orthogonal', 100, 'tanh'),\n",
        "    # ('orthogonal', 100, 'sin'),\n",
        "\n",
        "]\n",
        "\n",
        "save_path = 'training/mnist_v2/'\n",
        "checkpoint_path = 'training/checkpoints/'\n",
        "\n",
        "# Ensure directories exists\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "for init_type, num_layers, activation in combinations:\n",
        "    # Update the config dictionary with the current combination\n",
        "    config['init_type'] = init_type\n",
        "    config['num_layers'] = num_layers\n",
        "    config['activation'] = activation\n",
        "\n",
        "    # Instantiate the model with the updated configuration\n",
        "    model = MLPWithBatchNorm(\n",
        "        input_dim=1*28*28,  # Adjust for your dataset\n",
        "        output_dim=10,      # Number of classes in your dataset\n",
        "        num_layers=config['num_layers'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        norm_type=config['norm_type'],\n",
        "        mean_reduction=config['mean_reduction'],\n",
        "        activation=ACTIVATIONS[config['activation']],\n",
        "        save_hidden=False,\n",
        "        exponent=config['gain_exponent'],\n",
        "        order=config['order'],\n",
        "        force_factor=config['force_factor'],\n",
        "        bias=config['bias']\n",
        "    ).to(device)\n",
        "\n",
        "    model.reset_parameters(config['init_type'])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "    start_epoch = 1\n",
        "    df = []\n",
        "    checkpoint_file = os.path.join(checkpoint_path, f'checkpoint_d{num_layers}_{activation}_{init_type}.pt')\n",
        "\n",
        "    # Load checkpoint if it exists\n",
        "    if os.path.isfile(checkpoint_file):\n",
        "        checkpoint = torch.load(checkpoint_file)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        df = checkpoint['df']\n",
        "\n",
        "    progress_bar = tqdm(range(start_epoch, config['num_epochs'] + 1), desc=\"Training\")\n",
        "    for epoch in progress_bar:\n",
        "        train_loss, train_acc = train_one_epoch(model, trainloader, optimizer, criterion, device)\n",
        "        test_loss, test_acc = test_one_epoch(model, testloader, criterion, device)\n",
        "\n",
        "        df.append({\n",
        "            'epoch': epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'test_loss': test_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'test_acc': test_acc,\n",
        "        })\n",
        "        progress_bar.set_description(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'df': df,\n",
        "        }, checkpoint_file)\n",
        "\n",
        "    # Save the final results to CSV\n",
        "    results_df = pd.DataFrame(df)\n",
        "    save_name = f'mnist_d{num_layers}_{activation}_{init_type}.csv'\n",
        "    results_df.to_csv(os.path.join(save_path, save_name))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
